### 感知机



感知机是二分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别。感知机对应于输入控件（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降发对损失函数进行极小化，求得感知机模型。感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。感知机预测时用学习得到的感知机模型对新的输入实例进行分类。感知机也是神经网络和支持向量机的基础。

目标函数：

$f(x) = sign(\omega\cdot x + b)$

其中：

$sign(x) = \begin{cases}+1&x\geq0 \\ -1 & x<0\end{cases}$

损失函数：

$L(\omega,b) = -\sum\limits_{x_i\in M} y_i(\omega \cdot x_i + b)$

梯度：

$$\nabla_\omega L(\omega,b) = -\sum\limits_{x_i\in M}y_i x_i $$

$$\nabla_b L(\omega,b) = -\sum\limits_{x_i\in M}y_i $$

对训练集使用随机梯度下降法最小化损失函数。随机梯度下降法即每次随机选择一个样本点进行优化，具体公式如下：

$$\omega \leftarrow \omega + \eta x_i y_i$$

$$b \leftarrow b + \eta y_i$$

Python代码示例：

```python
for _x, _y in zip(self.x_train, self.y_train):
    _y = 1 if _y == 1 else -1
    y_p = np.dot(_x, self.W) + self.b
    if y_p * _y <= 0:
        self.W = self.W + self.lr * _y * _x
        self.b = self.b + self.lr * _y
        flag = True
```

不断迭代上述过程，直至训练集样本全部分类正确或者达到指定的迭代次数。

可以证明，经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。也就是说，当训练集线性可分时，感知机学习算法原始形式迭代是收敛的。但感知机学习算法存在许多解，这些解既依赖于初值的选择，也依赖于迭代过程中误分类点的选择顺序。为了得到唯一的超平面，需要对分离超平面增加约束条件，这就是线性支持向量机的思想。当训练数据线性不可分时，感知机学习算法不收敛，迭代结果会发生振荡。

使用梯度下降的方法求得的解不唯一，这可能会导致模型在训练集上能够全部判断正确，但在测试集上出现较大误差的情况。